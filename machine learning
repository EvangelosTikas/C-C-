// Activation function and its derivative
double sigmoid(double x) { return 1 / (1 + exp(-x)); }
double dSigmoid(double x) { return x * (1 â€” x); }
// Init all weights and biases between 0.0 and 1.0
double init_weight() { return ((double)rand())/((double)RAND_MAX); }
/*Our network will consist of a single hidden layer with 2 nodes and a single output layer node.
This is the minimal configuration for learning the XOR function: */
//defining dimensions
static const int numInputs = 2;
static const int numHiddenNodes = 2;
static const int numOutputs = 1;
double hiddenLayer[numHiddenNodes];
double outputLayer[numOutputs];
double hiddenLayerBias[numHiddenNodes];
double outputLayerBias[numOutputs];
double hiddenWeights[numInputs][numHiddenNodes];
double outputWeights[numHiddenNodes][numOutputs];
//Here is the training set:
static const int numTrainingSets = 4;
double training_inputs[numTrainingSets][numInputs] = { {0.0f,0.0f},{1.0f,0.0f},{0.0f,1.0f},{1.0f,1.0f} };
double training_outputs[numTrainingSets][numOutputs] = { {0.0f},{1.0f},{1.0f},{0.0f} };
